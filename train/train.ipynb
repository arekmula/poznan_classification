{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poznan_classification_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCrMaIQQC-Wa"
      },
      "source": [
        "## Utilities to load dataset and show images and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKHRCfyzB4xX"
      },
      "source": [
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def load_dataset(dataset_dir_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    print(\"Loading dataset\")\n",
        "    x, y = [], []\n",
        "    for i, class_dir in enumerate(sorted(dataset_dir_path.iterdir())):\n",
        "        for file in class_dir.iterdir():\n",
        "            img_file = cv2.imread(str(file))  # TODO: Check if reading as GRAYSCALE image isn't better\n",
        "            x.append(img_file)\n",
        "            y.append(i)\n",
        "\n",
        "    return np.asarray(x), np.asarray(y)\n",
        "\n",
        "\n",
        "def show_images_and_labels(images, labels):\n",
        "    for image, label in zip(images, labels):\n",
        "        cv2.imshow(\"image\", image)\n",
        "        print(label)\n",
        "        cv2.waitKey()"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hglfMhPZDKNR"
      },
      "source": [
        "## Function to process the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFj60yV4DHWl"
      },
      "source": [
        "def data_processing(x: np.ndarray) -> np.ndarray:\n",
        "    print(\"Processing data\")\n",
        "    images_resized = []\n",
        "    for image in x:\n",
        "        image_resized = cv2.resize(image, (1200, 1000))  # TODO: Check smaller sizes\n",
        "        images_resized.append(image_resized)\n",
        "\n",
        "    return np.asarray(images_resized)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq8Mzp1zDeEt"
      },
      "source": [
        "## Function to create vocabulary model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AKmXnc6DRIb"
      },
      "source": [
        "from sklearn import cluster\n",
        "import pickle\n",
        "\n",
        "def create_vocab_model(train_descriptors, nb_words=64):\n",
        "    print(\"Creating vocab model\")\n",
        "    kmeans = cluster.KMeans(n_clusters=nb_words, random_state=42)\n",
        "    kmeans.fit(train_descriptors)\n",
        "    # pickle.dump(kmeans, open(\"vocab_model.p\", \"wb\"))\n",
        "    return kmeans"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtqfjcQ5JD5R"
      },
      "source": [
        "## Function that converts image descriptors to histogram based on vocabulary model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1792NkjDJCJo"
      },
      "source": [
        "def convert_descriptor_to_histogram(descriptors, vocab_model, normalize=True) -> np.ndarray:\n",
        "    features_words = vocab_model.predict(descriptors)  # KMeans returns indexes of words\n",
        "    histogram = np.zeros(vocab_model.n_clusters, dtype=np.float32)\n",
        "    unique, counts = np.unique(features_words, return_counts=True)\n",
        "    histogram[unique] += counts\n",
        "    if normalize:\n",
        "        histogram /= histogram.sum()\n",
        "    return histogram"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcAMy51KDlYr"
      },
      "source": [
        "## Function that applies feature transform on image, and then create histogram from image descriptors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWnDZL8pDjkw"
      },
      "source": [
        "def apply_feature_transform(data: np.ndarray,\n",
        "                            feature_detector_descriptor,\n",
        "                            vocab_model\n",
        "                            ) -> np.ndarray:\n",
        "    print(\"Applying future transform\")\n",
        "    data_transformed = []\n",
        "    for image in data:\n",
        "        keypoints, image_descriptors = feature_detector_descriptor.detectAndCompute(image, None)\n",
        "        bow_features_histogram = convert_descriptor_to_histogram(image_descriptors, vocab_model)\n",
        "        data_transformed.append(bow_features_histogram)\n",
        "    return np.asarray(data_transformed)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbTweUKzEUli"
      },
      "source": [
        "# Main part of training\n",
        "## Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihL9dWFZESI0",
        "outputId": "5522b21c-3393-4b9e-f9ac-49f69ca9dfcd"
      },
      "source": [
        "import os\n",
        "\n",
        "data_path = Path('drive/MyDrive/train_poznan_classification/')  # You can change the path here\n",
        "data_path = os.getenv('DATA_PATH', data_path)  # Don't change that line\n",
        "images, labels = load_dataset(data_path)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8XMDXkKFV7I"
      },
      "source": [
        "## Processing images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbD4r74XFSuc",
        "outputId": "d0f01ac3-b0d3-444c-ceb5-9df654d7414c"
      },
      "source": [
        "  images = data_processing(images)\n"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uomtcmzYFoWl"
      },
      "source": [
        "## Spliting training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcZd-aumFj4P"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, train_size=0.8,\n",
        "                                                                            random_state=42, stratify=labels)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn1rHwfiGA3O"
      },
      "source": [
        "## Creating feature descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEl0BRyIGAY3"
      },
      "source": [
        "feature_detector_descriptor = cv2.AKAZE_create()  # TODO: Check different parameters\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKgXtXOFzzP"
      },
      "source": [
        "## Creating new vocabulary model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pZ0kh55F5Vx",
        "outputId": "40208784-3d90-4a34-8847-9e1b0132a33d"
      },
      "source": [
        "  train_descriptors = []\n",
        "  for image in train_images:\n",
        "      # TODO: Moze sprawdz maske binarna na srodku obrazu\n",
        "      # Drugim argumentem może być maska binarna, która służy do zawężenia obszaru z którego\n",
        "      # uzyskujemy punkty kluczowe/deskryptor – jako, że nam akurat zależy na całym obrazie,\n",
        "      # podaliśmy tam wartość None.\n",
        "      for descriptor in feature_detector_descriptor.detectAndCompute(image, None)[1]:\n",
        "          train_descriptors.append(descriptor)\n",
        "  vocab_model = create_vocab_model(train_descriptors)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating vocab model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ACpogYQGNd2"
      },
      "source": [
        "## Use created vocabulary model to apply feature transform on train images and test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Di-DASGVXY",
        "outputId": "325f241b-1faf-4856-9fc5-1d1fb7a2a424"
      },
      "source": [
        "# with Path('vocab_model.p').open('rb') as vocab_file:  # Don't change the path here\n",
        "#     vocab_model = pickle.load(vocab_file)\n",
        "\n",
        "X_train = apply_feature_transform(train_images, feature_detector_descriptor, vocab_model)\n",
        "y_train = train_labels\n",
        "\n",
        "X_test = apply_feature_transform(test_images, feature_detector_descriptor, vocab_model)\n",
        "y_test = test_labels"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying future transform\n",
            "Applying future transform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml8eSsOTY7ME",
        "outputId": "7a56bfcf-da65-469b-be34-19de2695f6f3"
      },
      "source": [
        "eval_path = Path('drive/MyDrive/test_poznan_classification/')  # You can change the path here\n",
        "eval_images, eval_labels = load_dataset(eval_path)\n",
        "eval_images = data_processing(eval_images)\n",
        "X_eval = apply_feature_transform(eval_images, feature_detector_descriptor, vocab_model)\n",
        "y_eval = eval_labels\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset\n",
            "Processing data\n",
            "Applying future transform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTS5Ew4nG2jy"
      },
      "source": [
        "## Create classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trbBuOw2G5n2"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# classifier = DecisionTreeClassifier()\n",
        "classifier = svm.SVC()"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdEJVJsLbcvA"
      },
      "source": [
        "## Search for the best params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C6tQ69ybb0K",
        "outputId": "6543fc9c-e315-461a-ff22-2d9d09001043"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "param_grid = {\n",
        "    \"C\": [1, 1.5, 2, 2.5, 5],\n",
        "    \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n",
        "    \"degree\": [3, 4, 5, 6],\n",
        "    \"gamma\": [\"scale\", \"auto\"]\n",
        "}\n",
        "k_fold = KFold(n_splits=5)\n",
        "\n",
        "grid_search = GridSearchCV(classifier, param_grid, cv=k_fold)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(grid_search.score(X_test, y_test))\n",
        "print(grid_search.score(X_eval, y_eval))\n",
        "print(grid_search.best_params_)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9642857142857143\n",
            "0.85\n",
            "{'C': 2.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6_3ZwV4G_RA"
      },
      "source": [
        "## Train the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOXeqxZIHBEb",
        "outputId": "8c9bde61-3321-4273-bc61-125b023684be"
      },
      "source": [
        "classifier.fit(X_train, y_train)\n",
        "# print(classifier.score(X_train, y_train))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRaF3f3JZ2QI",
        "outputId": "923e86b1-be00-44b0-f667-7c7531e158eb"
      },
      "source": [
        "print(classifier.score(X_test, y_test))\n",
        "print(classifier.score(X_eval, y_eval))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9642857142857143\n",
            "0.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgKwOqsHSbj1"
      },
      "source": [
        "## Save vocab model and classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KRiFCmfSPMQ"
      },
      "source": [
        "pickle.dump(vocab_model, open(\"vocab_model.p\", \"wb\"))\n",
        "pickle.dump(classifier, open(\"clf.p\", \"wb\"))\n"
      ],
      "execution_count": 156,
      "outputs": []
    }
  ]
}