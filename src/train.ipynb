{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopia notatnika poznan_classification_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCrMaIQQC-Wa"
      },
      "source": [
        "## Utilities to load dataset and show images and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKHRCfyzB4xX"
      },
      "source": [
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def load_dataset(dataset_dir_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    print(\"Loading dataset\")\n",
        "    x, y = [], []\n",
        "    for i, class_dir in enumerate(sorted(dataset_dir_path.iterdir())):\n",
        "        for file in class_dir.iterdir():\n",
        "            img_file = cv2.imread(str(file))  # TODO: Check if reading as GRAYSCALE image isn't better\n",
        "            x.append(img_file)\n",
        "            y.append(i)\n",
        "\n",
        "    return np.asarray(x), np.asarray(y)\n",
        "\n",
        "\n",
        "def show_images_and_labels(images, labels):\n",
        "    for image, label in zip(images, labels):\n",
        "        cv2.imshow(\"image\", image)\n",
        "        print(label)\n",
        "        cv2.waitKey()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hglfMhPZDKNR"
      },
      "source": [
        "## Function to process the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFj60yV4DHWl"
      },
      "source": [
        "def data_processing(x: np.ndarray) -> np.ndarray:\n",
        "    print(\"Processing data\")\n",
        "    images_resized = []\n",
        "    for image in x:\n",
        "        image_resized = cv2.resize(image, (800, 600))  # TODO: Check smaller sizes\n",
        "        images_resized.append(image_resized)\n",
        "\n",
        "    return np.asarray(images_resized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq8Mzp1zDeEt"
      },
      "source": [
        "## Function to create vocabulary model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AKmXnc6DRIb"
      },
      "source": [
        "from sklearn import cluster\n",
        "import pickle\n",
        "\n",
        "def create_vocab_model(train_descriptors, nb_words=64):\n",
        "    print(\"Creating vocab model\")\n",
        "    kmeans = cluster.KMeans(n_clusters=nb_words, random_state=42)\n",
        "    kmeans.fit(train_descriptors)\n",
        "    # pickle.dump(kmeans, open(\"vocab_model.p\", \"wb\"))\n",
        "    return kmeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtqfjcQ5JD5R"
      },
      "source": [
        "## Function that converts image descriptors to histogram based on vocabulary model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1792NkjDJCJo"
      },
      "source": [
        "def convert_descriptor_to_histogram(descriptors, vocab_model, normalize=True) -> np.ndarray:\n",
        "    features_words = vocab_model.predict(descriptors)  # KMeans returns indexes of words\n",
        "    histogram = np.zeros(vocab_model.n_clusters, dtype=np.float32)\n",
        "    unique, counts = np.unique(features_words, return_counts=True)\n",
        "    histogram[unique] += counts\n",
        "    if normalize:\n",
        "        histogram /= histogram.sum()\n",
        "    return histogram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcAMy51KDlYr"
      },
      "source": [
        "## Function that applies feature transform on image, and then create histogram from image descriptors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWnDZL8pDjkw"
      },
      "source": [
        "def apply_feature_transform(data: np.ndarray,\n",
        "                            feature_detector_descriptor,\n",
        "                            vocab_model\n",
        "                            ) -> np.ndarray:\n",
        "    print(\"Applying future transform\")\n",
        "    data_transformed = []\n",
        "    for image in data:\n",
        "        keypoints, image_descriptors = feature_detector_descriptor.detectAndCompute(image, None)\n",
        "        bow_features_histogram = convert_descriptor_to_histogram(image_descriptors, vocab_model)\n",
        "        data_transformed.append(bow_features_histogram)\n",
        "    return np.asarray(data_transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbTweUKzEUli"
      },
      "source": [
        "# Main part of training\n",
        "## Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihL9dWFZESI0",
        "outputId": "b81dbff3-9442-4ad1-9532-1d7e60ea4aa6"
      },
      "source": [
        "import os\n",
        "\n",
        "data_path = Path('drive/MyDrive/train_poznan_classification/')  # You can change the path here\n",
        "data_path = os.getenv('DATA_PATH', data_path)  # Don't change that line\n",
        "images, labels = load_dataset(data_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8XMDXkKFV7I"
      },
      "source": [
        "## Processing images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbD4r74XFSuc",
        "outputId": "acdb8fd9-4af6-489c-8ad0-8b36ae4722b9"
      },
      "source": [
        "  images = data_processing(images)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uomtcmzYFoWl"
      },
      "source": [
        "## Spliting training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcZd-aumFj4P"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, train_size=0.8,\n",
        "                                                                            random_state=42, stratify=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn1rHwfiGA3O"
      },
      "source": [
        "## Creating feature descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEl0BRyIGAY3"
      },
      "source": [
        "feature_detector_descriptor = cv2.AKAZE_create()  # TODO: Check different parameters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKgXtXOFzzP"
      },
      "source": [
        "## Creating new vocabulary model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pZ0kh55F5Vx",
        "outputId": "eecc4153-aaab-400f-af03-3bb065f8428e"
      },
      "source": [
        "  train_descriptors = []\n",
        "  for image in train_images:\n",
        "      # TODO: Moze sprawdz maske binarna na srodku obrazu\n",
        "      # Drugim argumentem może być maska binarna, która służy do zawężenia obszaru z którego\n",
        "      # uzyskujemy punkty kluczowe/deskryptor – jako, że nam akurat zależy na całym obrazie,\n",
        "      # podaliśmy tam wartość None.\n",
        "      for descriptor in feature_detector_descriptor.detectAndCompute(image, None)[1]:\n",
        "          train_descriptors.append(descriptor)\n",
        "  vocab_model = create_vocab_model(train_descriptors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating vocab model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ACpogYQGNd2"
      },
      "source": [
        "## Use created vocabulary model to apply feature transform on train images and test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Di-DASGVXY",
        "outputId": "f83a684e-4269-4449-c1dc-a78552ee9f2c"
      },
      "source": [
        "# with Path('vocab_model.p').open('rb') as vocab_file:  # Don't change the path here\n",
        "#     vocab_model = pickle.load(vocab_file)\n",
        "\n",
        "X_train = apply_feature_transform(train_images, feature_detector_descriptor, vocab_model)\n",
        "y_train = train_labels\n",
        "\n",
        "X_test = apply_feature_transform(test_images, feature_detector_descriptor, vocab_model)\n",
        "y_test = test_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying future transform\n",
            "Applying future transform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTS5Ew4nG2jy"
      },
      "source": [
        "## Create classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trbBuOw2G5n2"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# classifier = DecisionTreeClassifier()\n",
        "classifier = svm.SVC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6_3ZwV4G_RA"
      },
      "source": [
        "## Train the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOXeqxZIHBEb",
        "outputId": "2777ec18-f94d-4fcf-db0a-b08f7a431e12"
      },
      "source": [
        "classifier.fit(X_train, y_train)\n",
        "# print(classifier.score(X_train, y_train))\n",
        "print(classifier.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8928571428571429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KRiFCmfSPMQ"
      },
      "source": [
        "pickle.dump(vocab_model, open(\"vocab_model.p\", \"wb\"))\n",
        "pickle.dump(classifier, open(\"clf.p\", \"wb\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}